{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J06ejXnc9Mj7",
        "outputId": "e7d2f3dc-5c6e-40a1-be47-81910a43a6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XHAcMhelkZgV",
        "outputId": "5c4ca142-e609-4293-9357-970b972b1734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6g0TLML9Mj-"
      },
      "outputs": [],
      "source": [
        "class My_CBOW(nn.Module):\n",
        "    def __init__(self, vob_size:int, embed_dim:int, window_size:int):\n",
        "        super(My_CBOW,self).__init__()\n",
        "        self.inEmbedding = nn.Embedding(vob_size,embed_dim)\n",
        "      \n",
        "        size = embed_dim * window_size * 2;\n",
        "\n",
        "#        print(f\"size =={size}\")\n",
        "        # self.linear1 = nn.Linear(size, embed_dim)\n",
        "\n",
        "        self.outEmbedding = nn.Embedding(vob_size,embed_dim)\n",
        "\n",
        "    #    self.h = None\n",
        "        \n",
        "    \n",
        "    def makeH(self,input):\n",
        "        W = self.inEmbedding(input)\n",
        "        n = W.shape[-2]\n",
        "\n",
        "\n",
        "  #      print(f\"W.shape = {W.shape}\")\n",
        "        h = torch.sum(W,dim=-2) / n\n",
        "    #     x = W.view(input.shape[0],-1)\n",
        "\n",
        "\n",
        "    #  #   print(f\"x.shape = {x.shape}\")\n",
        "    #     h = self.linear1(x)\n",
        "    #     h = nn.functional.relu(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "    def forwardWithH(self,h,out):\n",
        "        W = self.outEmbedding(out)\n",
        "        score = torch.sum(h * W,dim = -1)\n",
        "        return score\n",
        "\n",
        "\n",
        "\n",
        "# ## 模型测试\n",
        "# cbow = My_CBOW(10,3,3)\n",
        "# cbow.to(device=device)\n",
        "# input = torch.Tensor([[1,2,5],[1,2,3],[6,2,3],[6,2,3]]).to(dtype=torch.int32,device = device)\n",
        "\n",
        "# ##正样本\n",
        "# h = cbow.makeH(input)\n",
        "\n",
        "# print(f\"shape{h.shape}\")\n",
        "# pout = Tensor([1,3,2,5]).to(device=device,dtype=int)\n",
        "# plable = torch.ones(pout.shape).to(device = device)\n",
        "# pscore = cbow.forwardWithH(h,pout).to(device = device)\n",
        "\n",
        "\n",
        "# ##负样本\n",
        "# nout = Tensor([1,0,1,7]).to(device=device,dtype=int)\n",
        "# nlable = torch.zeros_like(nout).to(device = device)\n",
        "# nsocre = cbow.forwardWithH(h,nout).to(device = device)\n",
        "\n",
        "\n",
        "# score = torch.cat((pscore,nsocre))\n",
        "# lables = torch.cat((plable,nlable)).to(device = device)\n",
        "# print(score)\n",
        "\n",
        "\n",
        "\n",
        "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "# loss = loss_fn(score,lables)\n",
        "\n",
        "# print(loss)\n",
        "\n",
        "# loss.backward()\n",
        "\n",
        "# input = Tensor([[1,2,5],[1,2,3],[6,2,3]],dtype=torch.int)\n",
        "# print(input)\n",
        "\n",
        "# cbow.makeH(input)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE5bPBNm9MkA",
        "outputId": "3617fab0-65db-4eac-a5a2-fef071dfa25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(929579, 10)\n",
            "[   5    6    7 ...  552  917 3196]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/ml/nlp/\")\n",
        "\n",
        "import numpy\n",
        "import pickle\n",
        "\n",
        "\n",
        "from common.util import create_contexts_target\n",
        "from dataset import ptb\n",
        "\n",
        "\n",
        "window_size = 5\n",
        "hidden_size =  100\n",
        "\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "context , target = create_contexts_target(corpus=corpus,window_size=window_size)\n",
        "\n",
        "print(context.shape)\n",
        "print(target)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfaOhb4L9MkC"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "GPU = device == \"cuda\"\n",
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "\n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "\n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "\n",
        "        if not GPU:\n",
        "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                p = self.word_p.copy()\n",
        "                target_idx = target[i]\n",
        "                p[target_idx] = 0\n",
        "                p /= p.sum()\n",
        "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
        "        else:\n",
        "            # 在用GPU(cupy）计算时，优先速度\n",
        "            # 有时目标词存在于负例中\n",
        "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
        "                                               replace=True, p=self.word_p)\n",
        "\n",
        "        return negative_sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model:My_CBOW, loss_fn , optimizer:torch.optim.Optimizer ,context : Tensor, target: Tensor, negativeTarget: Tensor):\n",
        "\n",
        "    model.zero_grad()\n",
        "    # model.train()\n",
        "    h = model.makeH(context)\n",
        "\n",
        "    p_lable = torch.ones_like(target)\n",
        "    \n",
        "\n",
        "    p_score = model.forwardWithH(h,target)\n",
        "\n",
        "    n_lable = torch.zeros_like(negativeTarget)\n",
        "    n_score = model.forwardWithH(h,negativeTarget)\n",
        "\n",
        "    score = torch.cat((p_score,n_score)).to(dtype=torch.float32)\n",
        "    lables = torch.cat((p_lable,n_lable)).to(dtype=torch.float32)\n",
        "    loss = loss_fn(score,lables)\n",
        "    loss.backward()\n",
        "\n",
        "    #optimizer.zero_grad()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "aYTHkWrwsx-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LwkUHXn9MkC"
      },
      "outputs": [],
      "source": [
        "train_context = torch.tensor(context).to(device=device)\n",
        "train_target = torch.tensor(target).to(device = device)\n",
        "\n",
        "# print(train_context.shape)\n",
        "# print(train_target.shape)\n",
        "\n",
        "# model = My_CBOW(vocab_size,hidden_size)\n",
        "# optimizer = torch.optim.Adam(model.parameters())\n",
        "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## 取十条负样本\n",
        "# #sampler = torch.utils.data.Sampler(data_source)\n",
        "\n",
        "# smapler = UnigramSampler(corpus,0.75,1)\n",
        "\n",
        "\n",
        "# print(target.shape)\n",
        "\n",
        "\n",
        "# batch_context = train_context[:10]\n",
        "# batch_target = train_target[:10]\n",
        "\n",
        "\n",
        "\n",
        "# negative = smapler.get_negative_sample(batch_target)\n",
        "# print(negative.shape)\n",
        "\n",
        "# negative = torch.from_numpy(negative).squeeze(dim=1)\n",
        "# ##model.train()\n",
        "\n",
        "# print(f\"batch_target_shape{batch_target.shape}\")\n",
        "# train(model,loss_fn,optimizer,batch_context,batch_target,negative)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYDnU9Dr9MkD",
        "outputId": "cc291d94-a896-4500-84b4-c879c87efc1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9295\n"
          ]
        }
      ],
      "source": [
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(loop)\n",
        "\n",
        "\n",
        "model = My_CBOW(vocab_size,hidden_size , window_size=window_size)\n",
        "model = model.to(device=device)\n",
        "\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "loop = train_target.shape[0] // batch_size\n",
        "\n",
        "\n",
        " \n",
        "smapler = UnigramSampler(corpus,0.75,5)\n",
        "\n",
        "\n",
        "loss = 0.0\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "id": "D_xTA3CTS_qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "print(f\"model weight : {model.inEmbedding.weight}\")\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss = 0.0\n",
        "    for i in range(loop):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "\n",
        "        b_context = train_context[start : end]\n",
        "        b_target =  train_target[start : end]\n",
        "\n",
        "        negative = smapler.get_negative_sample(b_target)\n",
        "\n",
        "        negatives = torch.from_numpy(negative.T).to(device= device)\n",
        "\n",
        "        for j in range(5):\n",
        "          negative = negatives[j]\n",
        "          loss += train(model,loss_fn,optimizer,b_context,b_target,negative)\n",
        "\n",
        "\n",
        "        # negative = smapler.get_negative_sample(b_target)\n",
        "        # negative = torch.from_numpy(negative).squeeze(dim=1).to(device= device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "        # loss += train(model,loss_fn,optimizer,b_context,b_target,negative)\n",
        "\n",
        "    print(f\"{epoch} / {max_epoch} : losss is {loss/batch_size}\")\n",
        "\n",
        "print(f\"model weight : {model.inEmbedding.weight}\")"
      ],
      "metadata": {
        "id": "V6HLiu93zK0w",
        "outputId": "6f908d2c-dacb-4dbe-b2fa-ddeff43ee246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model weight : Parameter containing:\n",
            "tensor([[-1.2032, -1.2957, -1.0738,  ..., -2.4929, -1.1856,  0.7025],\n",
            "        [ 1.5415, -0.2944,  0.1391,  ..., -1.0367,  1.2740,  0.8600],\n",
            "        [-0.0066,  3.2815, -1.9544,  ...,  1.4111,  1.0171, -0.4052],\n",
            "        ...,\n",
            "        [ 1.3879,  1.0282, -1.6729,  ...,  0.5263,  0.6183, -1.3632],\n",
            "        [-2.2907, -0.7267, -0.4769,  ...,  2.5100,  0.5578,  0.8489],\n",
            "        [ 0.5662,  0.5856,  1.2439,  ..., -1.2348, -0.8450, -0.2257]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "0 / 10 : losss is 109.37078857421875\n",
            "1 / 10 : losss is 108.23054504394531\n",
            "2 / 10 : losss is 107.28948974609375\n",
            "3 / 10 : losss is 106.69515991210938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from common.util import most_similar\n",
        "\n",
        "\n",
        "## CBOW 模型的评价\n",
        "\n",
        "print(model.inEmbedding.weight.shape)\n",
        "\n",
        "\n",
        "word_vecs = model.inEmbedding.weight.detach().to(device = \"cpu\").numpy()\n",
        "\n",
        "querys = ['you','year','car','toyota']\n",
        "\n",
        "for query in querys:\n",
        "  most_similar(query,word_to_id,id_to_word,word_vecs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-TqMSjmFH39",
        "outputId": "cfbd1dac-8f3d-4045-a869-a0135291b3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 100])\n",
            "\n",
            "[query] you\n",
            " we: 0.6175280809402466\n",
            " i: 0.603376030921936\n",
            " they: 0.5740002393722534\n",
            " fingers: 0.35948315262794495\n",
            " investors: 0.3581792116165161\n",
            "\n",
            "[query] year\n",
            " month: 0.8081380724906921\n",
            " week: 0.7605192065238953\n",
            " spring: 0.5713254809379578\n",
            " summer: 0.5119264721870422\n",
            " day: 0.47378748655319214\n",
            "\n",
            "[query] car\n",
            " programming: 0.3897450566291809\n",
            " auto: 0.384994775056839\n",
            " explosion: 0.37117552757263184\n",
            " desire: 0.36241674423217773\n",
            " guy: 0.3510245978832245\n",
            "\n",
            "[query] toyota\n",
            " mazda: 0.41037535667419434\n",
            " restructured: 0.3895409405231476\n",
            " fiat: 0.3817201256752014\n",
            " international: 0.3804283142089844\n",
            " redmond: 0.35148000717163086\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "fe85713bd73e29e553e0fe93aab7a5c1beebd30dbde9209efd93f425af696249"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('language')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "CBOW-improve.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}