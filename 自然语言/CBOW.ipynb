{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_contexts_target , convert_one_hot\n",
    "\n",
    "\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5 \n",
    "max_epoch = 1000\n",
    "\n",
    "text = \"you say good and i say hello.\"\n",
    "corpus , word_to_id,id_to_word = preprocess(text)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "(6, 7)\n",
      "(6, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "contexts , target = create_contexts_target(corpus,window_size=1)\n",
    "target_raw = target[...]\n",
    "\n",
    "print(contexts)\n",
    "\n",
    "target = convert_one_hot(target , vocab_size)\n",
    "contexts = convert_one_hot(contexts , vocab_size)\n",
    "\n",
    "print(target.shape)\n",
    "\n",
    "print(contexts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW(torch.nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size):\n",
    "        super().__init__()\n",
    "        # =  torch.nn.Linear()\n",
    "        self.in_layer = torch.nn.Linear(vocab_size,hidden_size,bias=False)\n",
    "        self.out_layer = torch.nn.Linear(hidden_size,vocab_size,bias=False)\n",
    "\n",
    "\n",
    "    def forward(self ,contexts ):\n",
    "        h0 = self.in_layer(contexts[:,0])\n",
    "        h1 = self.in_layer(contexts[:,1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer(h)\n",
    "        return score\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = torch.from_numpy(contexts).to(dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "target = torch.from_numpy(target_raw).to(dtype=torch.int64)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of SimpleCBOW(\n",
      "  (in_layer): Linear(in_features=7, out_features=5, bias=False)\n",
      "  (out_layer): Linear(in_features=5, out_features=7, bias=False)\n",
      ")>\n",
      "loss is 1.9407833814620972\n",
      "loss is 1.788273811340332\n",
      "loss is 1.558205008506775\n",
      "loss is 1.289618730545044\n",
      "loss is 1.0601271390914917\n",
      "loss is 0.8724930286407471\n",
      "loss is 0.7124750018119812\n",
      "loss is 0.5861650109291077\n",
      "loss is 0.4934132993221283\n",
      "loss is 0.42614904046058655\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model = SimpleCBOW(vocab_size,hidden_size)\n",
    "\n",
    "\n",
    "print(model.parameters)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model.train()\n",
    "\n",
    "\n",
    "for i in range(max_epoch):\n",
    "    pred = model.forward(contexts)\n",
    "    loss = loss_fn(pred,target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if(i % 100 == 0):\n",
    "        print(f\"loss is {loss}\")\n",
    "    \n",
    "\n",
    "\n",
    "#def train():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0487, -0.5184,  0.3021,  0.5584,  0.6050, -1.2696, -1.1866],\n",
      "        [-0.0821,  0.6399, -1.3454,  0.7083, -0.8887, -0.1416, -1.1437],\n",
      "        [-1.1492,  0.4787, -0.8763, -1.3454, -0.9806, -1.0305,  1.0655],\n",
      "        [-1.1220,  1.3032, -0.7259,  1.1195, -0.7389, -0.8858,  1.1869],\n",
      "        [-1.0125,  1.1471, -0.6093,  0.9583, -0.7870, -1.5254, -0.3196]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.in_layer.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CBOW的改进版，pytorch 实现\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "vob_size = 5\n",
    "embedding_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([[1,2],[2,1],[1,3]])\n",
    "\n",
    "embedding = torch.nn.Embedding(vob_size,embedding_dim)\n",
    "\n",
    "W = embedding(input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7367,  1.0526],\n",
      "         [ 0.1018, -2.0935]],\n",
      "\n",
      "        [[ 0.1018, -2.0935],\n",
      "         [-0.7367,  1.0526]],\n",
      "\n",
      "        [[-0.7367,  1.0526],\n",
      "         [-0.0055, -1.7715]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3175, -0.5204],\n",
      "        [-0.3175, -0.5204],\n",
      "        [-0.3711, -0.3594]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = 0.5 * torch.sum(W,dim=-2)\n",
    "\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0909, -1.2018],\n",
       "        [ 1.5689,  1.0046],\n",
       "        [ 1.5689,  1.0046]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 中心词，即可序测测的词\n",
    "\n",
    "out_embed = torch.nn.Embedding(vob_size,embedding_dim)\n",
    "out_put = torch.tensor([1,2,2])\n",
    "target_W = out_embed(out_put)\n",
    "\n",
    "target_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0288,  0.6254],\n",
      "        [-0.4981, -0.5228],\n",
      "        [-0.5822, -0.3611]], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.6543, -1.0209, -0.9433], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(target_W * h)\n",
    "\n",
    "\n",
    "\n",
    "out = torch.sum(target_W * h ,dim= -1)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6543, -1.0209, -0.9433], grad_fn=<SumBackward1>)\n",
      "tensor([-1.0209,  0.6543,  0.4657], grad_fn=<SumBackward1>)\n",
      "torch.Size([6])\n",
      "tensor(-1.0209, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### 模拟负样本\n",
    "\n",
    "negtive_put = torch.tensor([2,1,1])\n",
    "negtive_W = out_embed(negtive_put)\n",
    "negtive_out = torch.sum(negtive_W * h ,dim= -1)\n",
    "\n",
    "print(out)\n",
    "print(negtive_out)\n",
    "all_out = torch.cat((out,negtive_out))\n",
    "\n",
    "\n",
    "\n",
    "print(all_out.shape)\n",
    "# print(all_out[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6543, -1.0209, -0.9433], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pos_weight = torch.ones([3])\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "y = torch.tensor([1,1,1],dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(out)\n",
    "loss = loss_fn(all_out,y)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe85713bd73e29e553e0fe93aab7a5c1beebd30dbde9209efd93f425af696249"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('language')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
