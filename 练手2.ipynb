{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "344538b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://lulaoshi.info/machine-learning/linear-model/minimise-loss-function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self):\n",
    "        # the weight vector\n",
    "        self.W = None\n",
    "\n",
    "    def train(self, X, y, method='bgd', learning_rate=1e-2, num_iters=100, verbose=False):\n",
    "        \"\"\"\n",
    "        Train linear regression using batch gradient descent or stochastic gradient descent\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: training data, shape (num_of_samples x num_of_features), num_of_samples rows of training sample, each training sample has num_of_features-dimension features.\n",
    "        y: target, shape (num_of_samples, 1). \n",
    "        method: (string) 'bgd' for Batch Gradient Descent or 'sgd' for Stochastic Gradient Descent\n",
    "        learning_rate: (float) learning rate or alpha\n",
    "        num_iters: (integer) number of steps to iterate for optimization\n",
    "        verbose: (boolean) if True, print out the progress\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        losses_history: (list) of losses at each training iteration\n",
    "        \"\"\"\n",
    "        num_of_samples, num_of_features = X.shape\n",
    "\n",
    "        if self.W is None:\n",
    "            # initilize weights with values\n",
    "            # shape (num_of_features, 1)\n",
    "            self.W = np.random.randn(num_of_features, 1) * 0.001\n",
    "        losses_history = []\n",
    "\n",
    "        for i in range(num_iters):\n",
    "\n",
    "            if method == 'sgd':\n",
    "                # randomly choose a sample\n",
    "                idx = np.random.choice(num_of_samples)\n",
    "                loss, grad = self.loss_and_gradient(X[idx, np.newaxis], y[idx, np.newaxis])\n",
    "            else:\n",
    "                loss, grad = self.loss_and_gradient(X, y)\n",
    "            losses_history.append(loss)\n",
    "\n",
    "            # Update weights using matrix computing (vectorized)\n",
    "            self.W -= learning_rate * grad\n",
    "\n",
    "            if verbose and i % (num_iters / 10) == 0:\n",
    "                print('iteration %d / %d : loss %f' %(i, num_iters, loss))\n",
    "        return losses_history\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict value of y using trained weights\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: predict data, shape (num_of_samples x num_of_features), each row is a sample with num_of_features-dimension features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred_ys: predicted data, shape (num_of_samples, 1)\n",
    "        \"\"\"\n",
    "        pred_ys = X.dot(self.W)\n",
    "        return pred_ys\n",
    "\n",
    "\n",
    "    def loss_and_gradient(self, X, y, vectorized=True):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        The same as self.train function\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of two items (loss, gradient)\n",
    "        loss: (float)\n",
    "        gradient: (array) with respect to self.W \n",
    "        \"\"\"\n",
    "        if vectorized:\n",
    "            return linear_loss_grad_vectorized(self.W, X, y)\n",
    "        else:\n",
    "            return linear_loss_grad_for_loop(self.W, X, y)\n",
    "\n",
    "\n",
    "def linear_loss_grad_vectorized(W, X, y):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients with weights, vectorized version\n",
    "    \"\"\"\n",
    "    # vectorized implementation \n",
    "    num_of_samples = X.shape[0]\n",
    "    # (num_of_samples, num_of_features) * (num_of_features, 1)\n",
    "    f_mat = X.dot(W)\n",
    "\n",
    "    # (num_of_samples, 1) - (num_of_samples, 1)\n",
    "    diff = f_mat - y \n",
    "    loss =   np.sum(diff * diff)/num_of_samples\n",
    "    \n",
    "    # {(num_of_samples, 1).T dot (num_of_samples, num_of_features)}.T\n",
    "  #  gradient = ((diff.T).dot(X)).T\n",
    "    gradient = np.dot(X.T,diff) /num_of_samples\n",
    "    return (loss, gradient)\n",
    "\n",
    "\n",
    "def linear_loss_grad_for_loop(W, X, y):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients with weights, for loop version\n",
    "    \"\"\"\n",
    "    \n",
    "    # num_of_samples rows of training data\n",
    "    num_of_samples = X.shape[0]\n",
    "    \n",
    "    # num_of_samples columns of features\n",
    "    num_of_features = X.shape[1]\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # shape (num_of_features, 1) same with W\n",
    "    gradient = np.zeros_like(W) \n",
    "    \n",
    "    for i in range(num_of_samples):\n",
    "        X_i = X[i, :] # i-th sample from training data\n",
    "        f = 0\n",
    "        for j in range(num_of_features):\n",
    "            f += X_i[j] * W[j, 0]\n",
    "        diff = f - y[i, 0]\n",
    "        loss += np.power(diff, 2)\n",
    "        for j in range(num_of_features):\n",
    "            gradient[j, 0] += diff * X_i[j]\n",
    "            \n",
    "    loss = 1.0 / 2 * loss\n",
    "\n",
    "    return (loss, gradient)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc4d4795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.42404282]\n",
      " [ -0.873987  ]\n",
      " [ -2.74934807]\n",
      " [ -0.8399295 ]\n",
      " [ -0.78877784]\n",
      " [ -2.86842619]\n",
      " [ -4.3001756 ]\n",
      " [  7.34986157]\n",
      " [ 11.96124358]\n",
      " [ -7.25166141]\n",
      " [  0.34983773]\n",
      " [ -3.65561418]\n",
      " [ -2.7424915 ]\n",
      " [ -0.84360163]\n",
      " [  2.67004468]\n",
      " [ -4.79971037]\n",
      " [ -1.50465931]\n",
      " [ -0.79914875]\n",
      " [ -3.38245673]\n",
      " [ -1.09562372]\n",
      " [ -3.14420095]\n",
      " [  1.91753761]\n",
      " [  2.25410096]\n",
      " [ -3.63063017]\n",
      " [ -2.71305841]\n",
      " [  0.76646079]\n",
      " [ -7.34423679]\n",
      " [ -1.32243276]\n",
      " [ -0.96792529]\n",
      " [ -3.76161417]\n",
      " [ -1.03162163]\n",
      " [  1.20169436]\n",
      " [ -1.45874168]\n",
      " [  0.27626248]\n",
      " [ -5.72966832]\n",
      " [ 10.15432288]\n",
      " [  1.63250684]\n",
      " [  4.58829623]\n",
      " [  8.2436905 ]\n",
      " [ -7.21555785]\n",
      " [ -4.7617956 ]\n",
      " [ -0.83297179]\n",
      " [ -1.64450968]\n",
      " [ -1.61152045]\n",
      " [  1.83752979]\n",
      " [  1.02732348]\n",
      " [ -2.1947042 ]\n",
      " [  2.12598459]\n",
      " [  7.22280789]\n",
      " [  0.6976439 ]\n",
      " [  5.14998633]\n",
      " [  4.61549158]\n",
      " [ -1.39280113]\n",
      " [  3.02669322]\n",
      " [  3.21094576]\n",
      " [  1.42417595]\n",
      " [ -1.14536796]\n",
      " [  0.37920782]\n",
      " [ 10.24868581]\n",
      " [  2.4313054 ]\n",
      " [  2.2852372 ]\n",
      " [  3.45839257]\n",
      " [  0.40776439]\n",
      " [  8.58820111]\n",
      " [  8.22704126]\n",
      " [ -0.85720697]\n",
      " [-11.29631917]\n",
      " [ -2.32581951]\n",
      " [ -4.25524711]\n",
      " [ -4.76515556]\n",
      " [  0.54321932]\n",
      " [ -2.72003327]\n",
      " [ -0.14499605]\n",
      " [  0.03320551]\n",
      " [  8.9197559 ]\n",
      " [ -8.49867079]\n",
      " [ -0.35703473]\n",
      " [  1.09469243]\n",
      " [  2.20244886]\n",
      " [  0.88432882]\n",
      " [ -5.26648062]\n",
      " [ -2.40692119]\n",
      " [ -3.21057085]\n",
      " [-13.10969214]\n",
      " [ -3.20663275]\n",
      " [  4.55469116]\n",
      " [  0.63046918]\n",
      " [  5.82590628]\n",
      " [ -2.13295123]\n",
      " [  6.2222547 ]\n",
      " [ -0.26540004]\n",
      " [ -5.26328492]\n",
      " [ -4.13182231]\n",
      " [  2.02372336]\n",
      " [  3.10494146]\n",
      " [ -3.2193836 ]\n",
      " [  0.44100423]\n",
      " [  3.55000397]\n",
      " [  4.8919314 ]\n",
      " [  1.79685984]]\n",
      "train_x:\n",
      "[[ 16.48702423]\n",
      " [-69.13958953]\n",
      " [ 62.46711496]\n",
      " [ 89.35923698]\n",
      " [ 62.28538881]\n",
      " [ 48.01390474]\n",
      " [-20.03512194]\n",
      " [-50.40685012]\n",
      " [ 62.76490479]\n",
      " [ 99.69993237]\n",
      " [-34.70665987]\n",
      " [ 20.03308398]\n",
      " [ 64.53048475]\n",
      " [ 60.73094603]\n",
      " [-58.24933596]\n",
      " [-65.42736619]\n",
      " [ 89.78817254]\n",
      " [-40.22687203]\n",
      " [ 78.18956143]\n",
      " [-42.6543944 ]\n",
      " [-49.66017802]\n",
      " [-53.18972466]\n",
      " [ 59.970222  ]\n",
      " [ 87.68484919]\n",
      " [ 73.92331203]\n",
      " [-86.09028505]\n",
      " [-87.99127596]\n",
      " [ -8.0686705 ]\n",
      " [ -9.82051863]\n",
      " [ 28.61599965]\n",
      " [ 93.06058832]\n",
      " [-45.29368099]\n",
      " [-69.19553005]\n",
      " [-54.014957  ]\n",
      " [ 51.75875319]\n",
      " [ 41.71958475]\n",
      " [ 37.17196597]\n",
      " [ 56.49110827]\n",
      " [-34.44718387]\n",
      " [ -6.06613924]\n",
      " [ 51.63799159]\n",
      " [-16.38282505]\n",
      " [-63.53340271]\n",
      " [ 29.8594821 ]\n",
      " [-24.07126387]\n",
      " [ 21.25639169]\n",
      " [ 25.85007718]\n",
      " [-36.07759362]\n",
      " [-51.03428439]\n",
      " [-74.28290876]\n",
      " [ 86.29251059]\n",
      " [  6.10024179]\n",
      " [ 24.62264792]\n",
      " [-49.19248639]\n",
      " [-51.47988951]\n",
      " [-98.8987928 ]\n",
      " [ 61.43285989]\n",
      " [ 37.71721895]\n",
      " [ 68.51079906]\n",
      " [ 17.8482936 ]\n",
      " [-42.78691479]\n",
      " [ 38.35119336]\n",
      " [ 32.31934675]\n",
      " [-62.05239803]\n",
      " [ 22.98646325]\n",
      " [ 67.08408419]\n",
      " [ 59.46582643]\n",
      " [ 37.40130544]\n",
      " [-80.1106283 ]\n",
      " [-57.42422858]\n",
      " [-53.96629659]\n",
      " [ 26.5690095 ]\n",
      " [ 34.46909879]\n",
      " [ 83.87375875]\n",
      " [-68.16532243]\n",
      " [-54.32642228]\n",
      " [ 84.03177341]\n",
      " [ 15.07688861]\n",
      " [ 80.91888393]\n",
      " [-66.51880113]\n",
      " [-44.29942212]\n",
      " [-66.67221739]\n",
      " [-55.81890764]\n",
      " [-33.99275556]\n",
      " [ 76.00030731]\n",
      " [ 72.83641361]\n",
      " [ 81.23333694]\n",
      " [-95.19917156]\n",
      " [-16.23216181]\n",
      " [ 71.76387776]\n",
      " [ 92.58727623]\n",
      " [ 63.41391762]\n",
      " [ 47.41557713]\n",
      " [ 84.3605163 ]\n",
      " [-73.26855814]\n",
      " [-16.73422956]\n",
      " [  0.50484876]\n",
      " [-63.84367256]\n",
      " [ 38.39016909]\n",
      " [-73.56060173]]\n",
      "train_y:\n",
      "[[  89.85916397]\n",
      " [-346.57193464]\n",
      " [ 309.58622673]\n",
      " [ 445.95625541]\n",
      " [ 310.63816619]\n",
      " [ 237.20109753]\n",
      " [-104.47578531]\n",
      " [-244.68438906]\n",
      " [ 325.78576751]\n",
      " [ 491.24800046]\n",
      " [-173.18346161]\n",
      " [  96.50980572]\n",
      " [ 319.90993225]\n",
      " [ 302.81112854]\n",
      " [-288.57663514]\n",
      " [-331.93654131]\n",
      " [ 447.43620337]\n",
      " [-201.93350888]\n",
      " [ 387.56535043]\n",
      " [-214.36759571]\n",
      " [-251.44509106]\n",
      " [-264.03108569]\n",
      " [ 302.10521096]\n",
      " [ 434.79361579]\n",
      " [ 366.90350176]\n",
      " [-429.68496444]\n",
      " [-447.30061661]\n",
      " [ -41.66578527]\n",
      " [ -50.07051847]\n",
      " [ 139.3183841 ]\n",
      " [ 464.27131997]\n",
      " [-225.26671058]\n",
      " [-347.43639192]\n",
      " [-269.7985225 ]\n",
      " [ 253.06409763]\n",
      " [ 218.75224663]\n",
      " [ 187.49233669]\n",
      " [ 287.04383759]\n",
      " [-163.99222883]\n",
      " [ -37.54625405]\n",
      " [ 253.42816235]\n",
      " [ -82.74709702]\n",
      " [-319.31152325]\n",
      " [ 147.68589007]\n",
      " [-118.51878953]\n",
      " [ 107.30928195]\n",
      " [ 127.05568172]\n",
      " [-178.26198352]\n",
      " [-247.94861405]\n",
      " [-370.71689989]\n",
      " [ 436.61253928]\n",
      " [  35.11670051]\n",
      " [ 121.72043846]\n",
      " [-242.93573871]\n",
      " [-254.18850179]\n",
      " [-493.06978804]\n",
      " [ 306.01893148]\n",
      " [ 188.96530255]\n",
      " [ 352.80268113]\n",
      " [  91.67277341]\n",
      " [-211.64933676]\n",
      " [ 195.21435938]\n",
      " [ 162.00449812]\n",
      " [-301.67378907]\n",
      " [ 123.15935751]\n",
      " [ 334.56321396]\n",
      " [ 286.03281298]\n",
      " [ 184.6807077 ]\n",
      " [-404.8083886 ]\n",
      " [-291.88629848]\n",
      " [-269.28826365]\n",
      " [ 130.12501422]\n",
      " [ 172.20049792]\n",
      " [ 419.40199926]\n",
      " [-331.90685626]\n",
      " [-280.13078216]\n",
      " [ 419.80183234]\n",
      " [  76.47913548]\n",
      " [ 406.79686849]\n",
      " [-331.70967681]\n",
      " [-226.76359123]\n",
      " [-335.76800816]\n",
      " [-282.30510905]\n",
      " [-183.07346994]\n",
      " [ 376.79490379]\n",
      " [ 368.73675923]\n",
      " [ 406.79715389]\n",
      " [-470.16995153]\n",
      " [ -83.29376025]\n",
      " [ 365.04164349]\n",
      " [ 462.67098111]\n",
      " [ 311.80630315]\n",
      " [ 232.94606336]\n",
      " [ 423.82630486]\n",
      " [-363.23784925]\n",
      " [ -86.89053141]\n",
      " [   2.96524805]\n",
      " [-315.66835884]\n",
      " [ 196.84277682]\n",
      " [-366.00614883]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_x = np.random.uniform(-100,100,100)\n",
    "train_x = train_x.reshape(-1,1)\n",
    "\n",
    "\n",
    "num_of_sample , num_of_feature = train_x.shape\n",
    "\n",
    "noise = np.random.randn(num_of_sample,1) * 5\n",
    "print(noise)\n",
    "t_w = np.array([5]).reshape(1,1)\n",
    "\n",
    "m = LinearRegresstion()\n",
    "m.W = t_w\n",
    "train_y = m.predict(x=train_x) + noise\n",
    "\n",
    "print(\"train_x:\")\n",
    "print(train_x)\n",
    "print(\"train_y:\")\n",
    "print(train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550233c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60400414]]\n"
     ]
    }
   ],
   "source": [
    "##初始化参数\n",
    "init_W = np.random.randn(num_of_feature,1)\n",
    "num_iters = 100\n",
    "print(init_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24dbcece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 100000 : loss 85082.321693\n",
      "iteration 10000 / 100000 : loss 113.586604\n",
      "iteration 20000 / 100000 : loss 20.224283\n",
      "iteration 30000 / 100000 : loss 20.121698\n",
      "iteration 40000 / 100000 : loss 20.121585\n",
      "iteration 50000 / 100000 : loss 20.121585\n",
      "iteration 60000 / 100000 : loss 20.121585\n",
      "iteration 70000 / 100000 : loss 20.121585\n",
      "iteration 80000 / 100000 : loss 20.121585\n",
      "iteration 90000 / 100000 : loss 20.121585\n",
      "[[4.99693996]]\n"
     ]
    }
   ],
   "source": [
    "m = LinearRegression()\n",
    "m.train(train_x,train_y,method=\"\",learning_rate=1e-7,num_iters=100000,verbose=True)\n",
    "\n",
    "print(m.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc59636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e330e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
